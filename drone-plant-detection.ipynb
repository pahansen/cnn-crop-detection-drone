{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index based Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the Excessive green index, plant and backgrounds pixels are segemented. In addition, Otsu thresholding is used to determine the threshold for plants and background.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read single image for sample size\n",
    "image_mat = cv2.imread(f'../image.JPG')\n",
    "\n",
    "# Compute excessive green index (ExG) for every plant pixel\n",
    "exg_factors = np.full((image_mat.shape[0], image_mat.shape[1], image_mat.shape[2]), [1, -2, 1])\n",
    "segmentation = np.sum(image_mat*exg_factors, 2)\n",
    "# Normalize pixel values with a max value of 255\n",
    "segmentation = cv2.normalize(src=segmentation, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "# Use Otsu to identify threshold between plant and background pixels in grayscale mask\n",
    "ret, background_mask = cv2.threshold(segmentation, 0, 255, cv2.THRESH_OTSU)\n",
    "ret, plant_mask = cv2.threshold(background_mask, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "plant_mask = np.array(plant_mask, np.uint8)\n",
    "background_mask = np.array(background_mask, np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hough transformation to identify crop rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automatically extract training data from the drone images, crop rows are computed. Every plant that crosses the crop rows is identified as crop. Therefore, a hough transformation is used. As preparation, contours of the plants are extracted for efficient computation. Additionally, outlier plants are extracted by contour size because we assume all plants have roughly the same size. This reduces computional complexity even further and yields a clean picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(img, houghLines, color=[255, 255, 255], thickness=1):\n",
    "    \"\"\"Draw lines on the image which can be used to determine intersections between lines and plant contours.\n",
    "    \"\"\"\n",
    "    for line in houghLines:\n",
    "        for rho,theta in line:\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            x0 = a*rho\n",
    "            y0 = b*rho\n",
    "            x1 = int(x0 + 10000*(-b))\n",
    "            y1 = int(y0 + 10000*(a))\n",
    "            x2 = int(x0 - 10000*(-b))\n",
    "            y2 = int(y0 - 10000*(a))\n",
    "            cv2.line(img,(x1,y1),(x2,y2),color,thickness)\n",
    "            \n",
    "\n",
    "def draw_contours(contours, shape, color = (255, 255, 255), thickness = 1):\n",
    "    \"\"\"Utitlity function to draw contours on black image.\n",
    "    \"\"\"\n",
    "    black_mat = np.zeros((shape[0],shape[1]), np.uint8)\n",
    "    for c in contours:\n",
    "        cv2.drawContours(black_mat, [c], -1, color, thickness)\n",
    "    return black_mat\n",
    "\n",
    "def calculate_main_theta(lines):\n",
    "    \"\"\"Calculate main angle for crop rows based on hough lines.\n",
    "    \"\"\"\n",
    "    thetas = lines[:,0,1]\n",
    "    hist = np.histogram(thetas, 180, (0, np.pi))\n",
    "    index = hist[0].argmax()\n",
    "    return index * np.pi / 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of Hough Lines transformation\n",
    "rho_resolution = 1\n",
    "theta_resolution = np.pi / 180\n",
    "threshold = 120\n",
    "tolerance = np.pi / 180 * 1.5\n",
    "exclude_contours_factor = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract plant contours from image\n",
    "contours = cv2.findContours(plant_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = imutils.grab_contours(contours)\n",
    "# Identify outliers and remove from contours\n",
    "contour_areas = [cv2.contourArea(c) for c in contours]\n",
    "contour_areas = sorted(contour_areas)\n",
    "outlier_index = math.ceil(len(contour_areas) * exclude_contours_factor)\n",
    "clean_contour_areas = contour_areas[outlier_index:len(contour_areas)]\n",
    "clean_contours = [c for c in contours if (cv2.contourArea(c)) > clean_contour_areas[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify crop rows based on cleaned contours. Initially, contours are placed onto a black image which is used to identify rows. In order to extract acutal crop rows, the main angle of the hough lines is calculated. Only hough lines are saved which angle is around a specified threshold to the main angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw contours of plants\n",
    "black_image = np.zeros(plant_mask.shape, np.uint8)\n",
    "for c in clean_contours:\n",
    "    cv2.drawContours(black_image, [c], -1, (255, 255), 1)\n",
    "edges = cv2.Canny(black_image,50,150,apertureSize = 7)\n",
    "non_zero_edges_pixels = cv2.findNonZero(edges)\n",
    "# Compute hough lines\n",
    "lines = cv2.HoughLines(edges, rho_resolution , theta_resolution , threshold)\n",
    "hough_lines_image = np.zeros(plant_mask.shape, np.uint8)\n",
    "if lines is None:\n",
    "        print(\"No crop rows found.\")\n",
    "else:\n",
    "    main_theta = calculate_main_theta(lines)\n",
    "    min_theta = main_theta - tolerance\n",
    "    max_theta = main_theta + tolerance\n",
    "    good_lines = cv2.HoughLines(edges, rho_resolution , theta_resolution , threshold, min_theta=min_theta, max_theta=max_theta)\n",
    "    if good_lines is None:\n",
    "        print(\"No good crop rows found.\")\n",
    "    else:\n",
    "        draw_lines(hough_lines_image, good_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the identified crop rows, we extract plants that intersect with these rows. This way, training data can be extracted from all aerial images automatically and we save to label those by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_mask = draw_contours(clean_contours, hough_lines_image.shape)\n",
    "intersections = cv2.bitwise_and(contour_mask, hough_lines_image)\n",
    "intersection_points = cv2.findNonZero(intersections)\n",
    "crops_on_row_contours = []\n",
    "crops_out_of_row_contours = []\n",
    "for c in clean_contours:\n",
    "    contour_on_row = False\n",
    "    y_max = max(c[:,0,-1])\n",
    "    y_min = min(c[:,0,-1])\n",
    "    x_max = max(c[:,0,0])\n",
    "    x_min = min(c[:,0,0])\n",
    "    for intersection_point in intersection_points:\n",
    "        if not contour_on_row:\n",
    "            if (intersection_point[0][0] >= x_min and intersection_point[0][0] <= x_max \n",
    "                and intersection_point[0][1] >= y_min \n",
    "                and intersection_point[0][1] <= y_max):\n",
    "                    dist = cv2.pointPolygonTest(c,\n",
    "                                                (intersection_point[0][0],\n",
    "                                                 intersection_point[0][1]),\n",
    "                                                False)\n",
    "                    if dist == 0:\n",
    "                        crops_on_row_contours.append(c)\n",
    "                        contour_on_row = True\n",
    "    if not contour_on_row:\n",
    "        crops_out_of_row_contours.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    \"\"\"Define CNN architecture.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(192, kernel_size=5, activation='relu', padding= 'same', input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Conv2D(256, kernel_size=5, activation='relu', input_shape=(16, 16, 3)))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Conv2D(256, kernel_size=3, activation='relu', input_shape=(8, 8 , 3)))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation ='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation ='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation= 'softmax'))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
